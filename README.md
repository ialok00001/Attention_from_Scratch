# Attention from Scratch

We replicate the famous 2017 paper 'Attention Is All You Need', which revolutionized the course of AI forever.

In this, we have covered everything from an architectural point of view. From attention mechanism, to masked attention, to encoder and decoder blocks, upto positional encoding.
I hope this proves to be a stepping stone in increasing our knowledge about attention mechanism and its working.
I have also included a python file with these classes, so that one can directly create attention blocks using this module.
