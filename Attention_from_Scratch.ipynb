{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edIM39PEVtPs"
      },
      "source": [
        "# Attention Is All You Need"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZfD5NLsVxpS"
      },
      "source": [
        "Welcome!\n",
        "\n",
        "In this project, we will replicate the famous paper - [Attention Is All You Need](https://arxiv.org/pdf/1706.03762), from where the world of LLMs started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ru1igTswaqUQ"
      },
      "outputs": [],
      "source": [
        "import torch ## torch let's us create tensors and also provides helper functions\n",
        "import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n",
        "import torch.nn.functional as F # This gives us the softmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ktL6nu1bKYN"
      },
      "source": [
        "First, we need the attention class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0LuWlnJbrwy"
      },
      "source": [
        "This is the step which takes the query, key and value matrices, and calculates the scaled dot product attention (SDPA)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjUzeC6Jb7KI"
      },
      "source": [
        "$$Attention(Q, K, V) = Softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjxvBYo-cUDX"
      },
      "source": [
        "Note that, the KV cache technique is used at the time of inference for converting the inference time complexity from quadratic to linear, at the cost of added space complexity. It has nothing to do with the model's architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vxoTHl_ebIPC"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_k, d_v, d_model=2,\n",
        "                 row_dim=0,\n",
        "                 col_dim=1):\n",
        "        \"\"\"\n",
        "        Calculates the attention scores for the given query, key and value vectors\n",
        "\n",
        "        input:\n",
        "           d_k: dimension of query and key vectors (its important that they have same dimension due to the compatibility of the dot product)\n",
        "           d_v: dimension of value vector\n",
        "           d_model: dimension of input vectors of model\n",
        "           row_dim: axis for rows\n",
        "           col_dim: axis for columns\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.W_q = nn.Linear(in_features=d_model, out_features=d_k, bias=False)\n",
        "        self.W_k = nn.Linear(in_features=d_model, out_features=d_k, bias=False)\n",
        "        self.W_v = nn.Linear(in_features=d_model, out_features=d_v, bias=False)\n",
        "\n",
        "        self.row_dim = row_dim\n",
        "        self.col_dim = col_dim\n",
        "\n",
        "\n",
        "    # The only change from SelfAttention and attention is that\n",
        "    # we expect 3 sets of encodings to be passed in...\n",
        "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
        "        # ...and we pass those sets of encodings to the various weight matrices.\n",
        "        q = self.W_q(encodings_for_q)\n",
        "        k = self.W_k(encodings_for_k)\n",
        "        v = self.W_v(encodings_for_v)\n",
        "\n",
        "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
        "\n",
        "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
        "\n",
        "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
        "\n",
        "        attention_scores = torch.matmul(attention_percents, v)\n",
        "\n",
        "        return attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9HvwPRNdXQH"
      },
      "source": [
        "We also need the Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10IEPtbldpDx"
      },
      "source": [
        "At one layer, we can use multiple attention heads in parallel. This encourages the model to learn different and independent attention scores, leading to more rich context info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Qznvr-HwddLY"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_k, d_v,\n",
        "                 d_model=2,\n",
        "                 row_dim=0,\n",
        "                 col_dim=1,\n",
        "                 num_heads=1):\n",
        "        \"\"\"\n",
        "        Calculates multiple attention heads for the given query, key and value vectors\n",
        "\n",
        "        input:\n",
        "           d_k: dimension of query and key vectors (its important that they have same dimension due to the compatibility of the dot product)\n",
        "           d_v: dimension of value vector\n",
        "           d_model: dimension of input and output vectors of model\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        ## create a bunch of attention heads\n",
        "        self.heads = nn.ModuleList(\n",
        "            [Attention(d_k, d_v, d_model, row_dim, col_dim)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "        # We want to make sure the output has dimension d_model\n",
        "        self.out = nn.Linear(in_features=num_heads*d_v,\n",
        "                             out_features=d_model,\n",
        "                             bias=False)\n",
        "\n",
        "        self.col_dim = col_dim\n",
        "\n",
        "    def forward(self,\n",
        "                encodings_for_q,\n",
        "                encodings_for_k,\n",
        "                encodings_for_v,\n",
        "                mask=None):\n",
        "\n",
        "        ## run the data through all of the attention heads\n",
        "        return self.out(torch.cat([head(encodings_for_q,\n",
        "                               encodings_for_k,\n",
        "                               encodings_for_v,\n",
        "                               mask)\n",
        "                          for head in self.heads], dim=self.col_dim))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxNIlbAcy3YN"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M98FqFSewAx"
      },
      "source": [
        "This is where we combine the above blocks and create the encoder layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3ZOcdOme7Ar"
      },
      "source": [
        "We can have multiple encoder layers connected in series. Fo this, we will first make an encoder layer and then repeat that several times to make an encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o0W8MVRPy2_u"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_k=64, d_v=64, d_model=512, row_dim=0, col_dim=1, num_heads=8, p_drop=0.1):\n",
        "\n",
        "    \"\"\"\n",
        "    This creates a single encoder layer, with multi-head attention and feed forward layers.\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.multihead = MultiHeadAttention(d_k, d_v, d_model, row_dim, col_dim, num_heads)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.ff1 = nn.Linear(in_features = d_model, out_features = d_model, bias = True)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.ff2 = nn.Linear(in_features=d_model, out_features = d_model, bias = False)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(p=p_drop)\n",
        "\n",
        "  def forward(self, encodings, mask = None):\n",
        "    x = self.multihead(encodings, encodings, encodings, mask)\n",
        "    x = self.norm1(x + encodings)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    y = self.ff1(x)\n",
        "    y = self.relu(y)\n",
        "    y = self.ff2(y)\n",
        "    y = self.norm2(y + x)\n",
        "    y = self.dropout(y)\n",
        "\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6uiie9Bs200G"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, d_k=64, d_v=64, d_model=512, row_dim=0, col_dim=1, num_heads=8, p_drop=0.1, N = 6):\n",
        "    \"\"\"\n",
        "    This repeats the encoder layer N times.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = nn.ModuleList([EncoderLayer(d_k, d_v, d_model, row_dim, col_dim, num_heads, p_drop)\n",
        "                                 for _ in range(N)])\n",
        "    self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, encodings, mask = None):\n",
        "    for layer in self.layers:\n",
        "      encodings = layer(encodings, mask)\n",
        "\n",
        "    return self.norm(encodings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg5c3aFbfbCk"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yplddMVt3kH7"
      },
      "source": [
        "Now the decoder. This is same as Encoder, but an additional 'masked' multi-head attention and a feed-forward network is added to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oyEIVmtq3luO"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_k=64, d_v=64, d_model=512, row_dim=0, col_dim=1, num_heads=8, p_drop=0.1):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.masked_multihead = MultiHeadAttention(d_k, d_v, d_model, row_dim, col_dim, num_heads)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.multihead = MultiHeadAttention(d_k, d_v, d_model, row_dim, col_dim, num_heads)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.ff1 = nn.Linear(in_features = d_model, out_features=d_model, bias = True)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.ff2 = nn.Linear(in_features=d_model, out_features=d_model)\n",
        "    self.norm3 = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(p=p_drop)\n",
        "    self.row_dim = row_dim\n",
        "    self.col_dim = col_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "  def forward(self, encoder_encodings, decoder_encodings, mask = None):\n",
        "    x = self.masked_multihead(decoder_encodings, decoder_encodings, decoder_encodings, mask)\n",
        "    x = self.norm1(x + decoder_encodings)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    y = self.multihead(x, encoder_encodings, encoder_encodings, None)\n",
        "    y = self.norm2(y + x)\n",
        "    y = self.dropout(y)\n",
        "\n",
        "    z = self.ff1(y)\n",
        "    z = self.relu(z)\n",
        "    z = self.ff2(z)\n",
        "    z = self.norm3(z + y)\n",
        "    z = self.dropout(z)\n",
        "\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xibyggdL86Do"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, d_k=64, d_v=64, d_model=512, row_dim=0, col_dim=1, num_heads=8, p_drop=0.1, N = 6):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = nn.ModuleList([DecoderLayer(d_k, d_v, d_model, row_dim, col_dim, num_heads, p_drop)\n",
        "                                 for _ in range(N)])\n",
        "    self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, encoder_encodings, decoder_encodings, mask = None):\n",
        "    for layer in self.layers:\n",
        "      decoder_encodings = layer(encoder_encodings, decoder_encodings, mask)\n",
        "\n",
        "    return self.norm(decoder_encodings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHAVemiWf0yc"
      },
      "source": [
        "That's the complete code for Encoder-Decoder architecture for attention. The only thing left is the embedding layer and the positional encoding. We can add the sinusoidal vectors to input vectors. For the input embedding, that can be generated using a feed-forward network from one-hot encoded vocabulary to a vector of size d_model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6txGVkCCCpjy"
      },
      "source": [
        "Let's test the above framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EjrI-SHOCoqX"
      },
      "outputs": [],
      "source": [
        "my_encoder = Encoder()\n",
        "my_decoder = Decoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4apGsJ07x_4O",
        "outputId": "2887a241-fce8-421e-e6f4-966b7ccb97fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([25, 512])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn((25, 512))\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_vAhAe_-66nd"
      },
      "outputs": [],
      "source": [
        "y = my_encoder(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr1zkEOw8ODK",
        "outputId": "fc571d31-6400-44a2-893a-611d71a19ff3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([25, 512])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXUdSj8W8SBI",
        "outputId": "5ff39913-f1c6-4fae-b3a9-924674560a08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([7, 512])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w = torch.randn((7, 512))\n",
        "w.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4_KALxj9HVq",
        "outputId": "8a57efc8-67e6-489a-8423-785a6cc702d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[False,  True,  True,  True,  True,  True,  True],\n",
              "        [False, False,  True,  True,  True,  True,  True],\n",
              "        [False, False, False,  True,  True,  True,  True],\n",
              "        [False, False, False, False,  True,  True,  True],\n",
              "        [False, False, False, False, False,  True,  True],\n",
              "        [False, False, False, False, False, False,  True],\n",
              "        [False, False, False, False, False, False, False]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# prepare the mask\n",
        "mask = torch.tril(torch.ones(7, 7))\n",
        "mask = mask == 0\n",
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tdlam_xu8Z3O"
      },
      "outputs": [],
      "source": [
        "z = my_decoder(y, w, mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlbgwMeJNkKS",
        "outputId": "d4f25082-fcbb-42dd-e178-b7b8af5be334"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([7, 512])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2zkNlguCunA",
        "outputId": "be57399b-c3e0-434e-e000-fc321fd72ec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wz_RAhPYOabK"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rETe7RUUOdb6",
        "outputId": "25883caa-e0f9-4a10-f6c3-674aecd6e390"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "======================================================================\n",
              "Layer (type:depth-idx)                        Param #\n",
              "======================================================================\n",
              "Encoder                                       --\n",
              "├─ModuleList: 1-1                             --\n",
              "│    └─EncoderLayer: 2-1                      --\n",
              "│    │    └─MultiHeadAttention: 3-1           1,048,576\n",
              "│    │    └─LayerNorm: 3-2                    1,024\n",
              "│    │    └─Linear: 3-3                       262,656\n",
              "│    │    └─ReLU: 3-4                         --\n",
              "│    │    └─Linear: 3-5                       262,144\n",
              "│    │    └─LayerNorm: 3-6                    1,024\n",
              "│    │    └─Dropout: 3-7                      --\n",
              "│    └─EncoderLayer: 2-2                      --\n",
              "│    │    └─MultiHeadAttention: 3-8           1,048,576\n",
              "│    │    └─LayerNorm: 3-9                    1,024\n",
              "│    │    └─Linear: 3-10                      262,656\n",
              "│    │    └─ReLU: 3-11                        --\n",
              "│    │    └─Linear: 3-12                      262,144\n",
              "│    │    └─LayerNorm: 3-13                   1,024\n",
              "│    │    └─Dropout: 3-14                     --\n",
              "│    └─EncoderLayer: 2-3                      --\n",
              "│    │    └─MultiHeadAttention: 3-15          1,048,576\n",
              "│    │    └─LayerNorm: 3-16                   1,024\n",
              "│    │    └─Linear: 3-17                      262,656\n",
              "│    │    └─ReLU: 3-18                        --\n",
              "│    │    └─Linear: 3-19                      262,144\n",
              "│    │    └─LayerNorm: 3-20                   1,024\n",
              "│    │    └─Dropout: 3-21                     --\n",
              "│    └─EncoderLayer: 2-4                      --\n",
              "│    │    └─MultiHeadAttention: 3-22          1,048,576\n",
              "│    │    └─LayerNorm: 3-23                   1,024\n",
              "│    │    └─Linear: 3-24                      262,656\n",
              "│    │    └─ReLU: 3-25                        --\n",
              "│    │    └─Linear: 3-26                      262,144\n",
              "│    │    └─LayerNorm: 3-27                   1,024\n",
              "│    │    └─Dropout: 3-28                     --\n",
              "│    └─EncoderLayer: 2-5                      --\n",
              "│    │    └─MultiHeadAttention: 3-29          1,048,576\n",
              "│    │    └─LayerNorm: 3-30                   1,024\n",
              "│    │    └─Linear: 3-31                      262,656\n",
              "│    │    └─ReLU: 3-32                        --\n",
              "│    │    └─Linear: 3-33                      262,144\n",
              "│    │    └─LayerNorm: 3-34                   1,024\n",
              "│    │    └─Dropout: 3-35                     --\n",
              "│    └─EncoderLayer: 2-6                      --\n",
              "│    │    └─MultiHeadAttention: 3-36          1,048,576\n",
              "│    │    └─LayerNorm: 3-37                   1,024\n",
              "│    │    └─Linear: 3-38                      262,656\n",
              "│    │    └─ReLU: 3-39                        --\n",
              "│    │    └─Linear: 3-40                      262,144\n",
              "│    │    └─LayerNorm: 3-41                   1,024\n",
              "│    │    └─Dropout: 3-42                     --\n",
              "├─LayerNorm: 1-2                              1,024\n",
              "======================================================================\n",
              "Total params: 9,453,568\n",
              "Trainable params: 9,453,568\n",
              "Non-trainable params: 0\n",
              "======================================================================"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(my_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYHS6VmhOfX-",
        "outputId": "2dbc7c7f-961b-4375-9cde-2756c0229139"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "======================================================================\n",
              "Layer (type:depth-idx)                        Param #\n",
              "======================================================================\n",
              "Decoder                                       --\n",
              "├─ModuleList: 1-1                             --\n",
              "│    └─DecoderLayer: 2-1                      --\n",
              "│    │    └─MultiHeadAttention: 3-1           1,048,576\n",
              "│    │    └─LayerNorm: 3-2                    1,024\n",
              "│    │    └─MultiHeadAttention: 3-3           1,048,576\n",
              "│    │    └─LayerNorm: 3-4                    1,024\n",
              "│    │    └─Linear: 3-5                       262,656\n",
              "│    │    └─ReLU: 3-6                         --\n",
              "│    │    └─Linear: 3-7                       262,656\n",
              "│    │    └─LayerNorm: 3-8                    1,024\n",
              "│    │    └─Dropout: 3-9                      --\n",
              "│    └─DecoderLayer: 2-2                      --\n",
              "│    │    └─MultiHeadAttention: 3-10          1,048,576\n",
              "│    │    └─LayerNorm: 3-11                   1,024\n",
              "│    │    └─MultiHeadAttention: 3-12          1,048,576\n",
              "│    │    └─LayerNorm: 3-13                   1,024\n",
              "│    │    └─Linear: 3-14                      262,656\n",
              "│    │    └─ReLU: 3-15                        --\n",
              "│    │    └─Linear: 3-16                      262,656\n",
              "│    │    └─LayerNorm: 3-17                   1,024\n",
              "│    │    └─Dropout: 3-18                     --\n",
              "│    └─DecoderLayer: 2-3                      --\n",
              "│    │    └─MultiHeadAttention: 3-19          1,048,576\n",
              "│    │    └─LayerNorm: 3-20                   1,024\n",
              "│    │    └─MultiHeadAttention: 3-21          1,048,576\n",
              "│    │    └─LayerNorm: 3-22                   1,024\n",
              "│    │    └─Linear: 3-23                      262,656\n",
              "│    │    └─ReLU: 3-24                        --\n",
              "│    │    └─Linear: 3-25                      262,656\n",
              "│    │    └─LayerNorm: 3-26                   1,024\n",
              "│    │    └─Dropout: 3-27                     --\n",
              "│    └─DecoderLayer: 2-4                      --\n",
              "│    │    └─MultiHeadAttention: 3-28          1,048,576\n",
              "│    │    └─LayerNorm: 3-29                   1,024\n",
              "│    │    └─MultiHeadAttention: 3-30          1,048,576\n",
              "│    │    └─LayerNorm: 3-31                   1,024\n",
              "│    │    └─Linear: 3-32                      262,656\n",
              "│    │    └─ReLU: 3-33                        --\n",
              "│    │    └─Linear: 3-34                      262,656\n",
              "│    │    └─LayerNorm: 3-35                   1,024\n",
              "│    │    └─Dropout: 3-36                     --\n",
              "│    └─DecoderLayer: 2-5                      --\n",
              "│    │    └─MultiHeadAttention: 3-37          1,048,576\n",
              "│    │    └─LayerNorm: 3-38                   1,024\n",
              "│    │    └─MultiHeadAttention: 3-39          1,048,576\n",
              "│    │    └─LayerNorm: 3-40                   1,024\n",
              "│    │    └─Linear: 3-41                      262,656\n",
              "│    │    └─ReLU: 3-42                        --\n",
              "│    │    └─Linear: 3-43                      262,656\n",
              "│    │    └─LayerNorm: 3-44                   1,024\n",
              "│    │    └─Dropout: 3-45                     --\n",
              "│    └─DecoderLayer: 2-6                      --\n",
              "│    │    └─MultiHeadAttention: 3-46          1,048,576\n",
              "│    │    └─LayerNorm: 3-47                   1,024\n",
              "│    │    └─MultiHeadAttention: 3-48          1,048,576\n",
              "│    │    └─LayerNorm: 3-49                   1,024\n",
              "│    │    └─Linear: 3-50                      262,656\n",
              "│    │    └─ReLU: 3-51                        --\n",
              "│    │    └─Linear: 3-52                      262,656\n",
              "│    │    └─LayerNorm: 3-53                   1,024\n",
              "│    │    └─Dropout: 3-54                     --\n",
              "├─LayerNorm: 1-2                              1,024\n",
              "======================================================================\n",
              "Total params: 15,754,240\n",
              "Trainable params: 15,754,240\n",
              "Non-trainable params: 0\n",
              "======================================================================"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(my_decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpesZ6tKCseN"
      },
      "source": [
        "## Positional Encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ohQbwHILOlrM"
      },
      "outputs": [],
      "source": [
        "def pos_encodings(token_len, d_model):\n",
        "  pos_enc = torch.zeros((token_len, d_model), dtype = torch.float32)\n",
        "  for pos in range(token_len):\n",
        "    for i in range(d_model):\n",
        "      if i % 2 == 0:\n",
        "        pos_enc[pos, i] = torch.sin(torch.tensor(pos / (10000 ** (i / d_model))))\n",
        "      else:\n",
        "        pos_enc[pos, i] = torch.cos(torch.tensor(pos / (10000 ** ((i - 1) / d_model))))\n",
        "\n",
        "  return pos_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvyNRoOxjjkf",
        "outputId": "53bdac62-a735-482f-fc56-08d9b9e1170c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([24, 512])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn((24, 512))\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED1xJ92zjoM6",
        "outputId": "e64d2a39-d4b6-4233-bd05-7fc80f0b464b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([24, 512])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = x + pos_encodings(24, 512)\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "SFPyXonBkQcj"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "59AMhZdIs-0B"
      },
      "outputs": [],
      "source": [
        "mask = torch.tril(torch.ones(10, 10))\n",
        "mask = mask == 0\n",
        "\n",
        "def testing(d_k=64, d_v=64, d_model=512, row_dim=0, col_dim=1, num_heads=8, p_drop=0.1, N = 6):\n",
        "  start = time()\n",
        "  my_encoder = Encoder(d_k, d_v, d_model, row_dim, col_dim, num_heads, p_drop, N)\n",
        "  my_decoder = Decoder(d_k, d_v, d_model, row_dim, col_dim, num_heads, p_drop, N)\n",
        "\n",
        "  x = torch.randn((5, d_model))\n",
        "  x = x + pos_encodings(5, d_model)\n",
        "\n",
        "  y = my_encoder(x)\n",
        "  w = torch.randn((10, d_model))\n",
        "  w = w + pos_encodings(10, d_model)\n",
        "\n",
        "  z = my_decoder(y, w, mask)\n",
        "\n",
        "  return time() - start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjWCzGgNti7A",
        "outputId": "eef86ae7-53af-48c6-f5c1-5410824d9982"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.3299205303192139"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mREPlgwEtmC-"
      },
      "outputs": [],
      "source": [
        "d_k = [3, 4, 12]\n",
        "d_v = [5, 10, 13]\n",
        "d_model = [8, 9, 17]\n",
        "num_heads = [1, 2, 6, 11]\n",
        "N = [1, 5, 12]\n",
        "\n",
        "params = {'d_k': d_k,\n",
        "          'd_v': d_v,\n",
        "          'd_model': d_model,\n",
        "          'row_dim': [0],\n",
        "          'col_dim': [1],\n",
        "          'num_heads': num_heads,\n",
        "          'N': N}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uwGu-bWuHzu",
        "outputId": "7399b7f7-3be7-4646-b634-95508bdabdad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "324"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(d_k) * len(d_v) * len(d_model) * len(num_heads) * len(N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYLUAooLvr9t",
        "outputId": "fe713e7a-bd64-40fc-9fbd-9f00666bcd9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing architecture for different values: 324it [00:24, 13.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time -> 23.192572832107544 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "\n",
        "keys   = list(params.keys())\n",
        "values = [params[k] for k in keys]\n",
        "\n",
        "total_time = 0\n",
        "\n",
        "for combo in tqdm(product(*values), desc = 'Testing architecture for different values'):\n",
        "\n",
        "    arg_dict = dict(zip(keys, combo))\n",
        "\n",
        "    result = testing(**arg_dict)\n",
        "    total_time += result\n",
        "\n",
        "print(f\"Total time -> {total_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "d0spqpBL3_9z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
